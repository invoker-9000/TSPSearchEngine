Gradient Descent in Machine Learning - Javatpoint ? SCROLL TO TOP Home AI Machine Learning DBMS Java Blockchain Control System Selenium HTML CSS JavaScript DS jQuery Quiz Interview Q Comment Forum Machine Learning Machine Learning Tutorial Machine Learning Applications Life cycle of Machine Learning Install Anaconda & Python AI vs Machine Learning How to Get Datasets Data Preprocessing Supervised Machine Learning Unsupervised Machine Learning Supervised vs Unsupervised Learning Supervised Learning Regression Analysis Linear Regression Simple Linear Regression Multiple Linear Regression Backward Elimination Polynomial Regression Classification Classification Algorithm Logistic Regression K-NN Algorithm Support Vector Machine Algorithm Na?ve Bayes Classifier Miscellaneous Classification vs Regression Linear Regression vs Logistic Regression Decision Tree Classification Algorithm Random Forest Algorithm Clustering in Machine Learning Hierarchical Clustering in Machine Learning K-Means Clustering Algorithm Apriori Algorithm in Machine Learning Association Rule Learning Confusion Matrix Cross-Validation Data Science vs Machine Learning Machine Learning vs Deep Learning Dimensionality Reduction Technique Machine Learning Algorithms Overfitting & Underfitting Principal Component Analysis What is P-Value Regularization in Machine Learning Examples of Machine Learning Semi-Supervised Learning Essential Mathematics for Machine Learning Overfitting in Machine Learning Types of Encoding Techniques Feature Selection Techniques in Machine Learning Bias and Variance in Machine Learning Machine Learning Tools Prerequisites for Machine Learning Gradient Descent in Machine Learning Machine Learning Experts Salary in India Machine Learning Models Machine Learning Books Linear Algebra for Machine learning Types of Machine Learning Related Tutorials Tensorflow Tutorial PyTorch Tutorial Data Science Tutorial AI Tutorial NLP Tutorial Reinforcement Learning Interview Questions Machine learning Interview next ? ? prev Gradient Descent in Machine Learning Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning models by means of minimizing errors between actual and expected results. Further, gradient descent is also used to train Neural Networks. In mathematical terminology, Optimization algorithm refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. Similarly, in machine learning, optimization is the task of minimizing the cost function parameterized by the model's parameters. The main objective of gradient descent is to minimize the convex function using iteration of parameter updates. Once these machine learning models are optimized, these models can be used as powerful tools for Artificial Intelligence and various computer science applications. In this tutorial on Gradient Descent in Machine Learning, we will learn in detail about gradient descent, the role of cost functions specifically as a barometer within Machine Learning, types of gradient descents, learning rates, etc. What is Gradient Descent or Steepest Descent? Gradient descent was initially discovered by "Augustin-Louis Cauchy" in mid of 18th century. Gradient Descent is defined as one of the most commonly used iterative optimization algorithms of machine learning to train the machine learning and deep learning models. It helps in finding the local minimum of a function. The best way to define the local minimum or local maximum of a function using gradient descent is as follows: If we move towards a negative gradient or away from the gradient of the function at the current point, it will give the local minimum of that function. Whenever we move towards a positive gradient or towards the gradient of the function at the current point, we will get the local maximum of that function. This entire procedure is known as Gradient Ascent, which is also known as steepest descent. The main objective of using a gradient descent algorithm is to minimize the cost function using iteration. To achieve this goal, it performs two steps iteratively: Calculates the first-order derivative of the function to compute the gradient or slope of that function. Move away from the direction of the gradient, which means slope increased from the current point by alpha times, where Alpha is defined as Learning Rate. It is a tuning parameter in the optimization process which helps to decide the length of the steps. What is Cost-function? The cost function is defined as the measurement of difference or error between actual values and expected values at the current position and present in the form of a single real number. It helps to increase and improve machine learning efficiency by providing feedback to this model so that it can minimize error and find the local or global minimum. Further, it continuously iterates along the direction of the negative gradient until the cost function approaches zero. At this steepest descent point, the model will stop learning further. Although cost function and loss function are considered synonymous, also there is a minor difference between them. The slight difference between the loss function and the cost function is about the error within the training of machine learning models, as loss function refers to the error of one training example, while a cost function calculates the average error across an entire training set. The cost function is calculated after making a hypothesis with initial parameters and modifying these parameters using gradient descent algorithms over known data to reduce the cost function. Hypothesis: Parameters: Cost function: Goal: How does Gradient Descent work? Before starting the working principle of gradient descent, we should know some basic concepts to find out the slope of a line from linear regression. The equation for simple linear regression is given as: 
       Y=mX+c
 Where 'm' represents the slope of the line, and 'c' represents the intercepts on the y-axis. The starting point(shown in above fig.) is used to evaluate the performance as it is considered just as an arbitrary point. At this starting point, we will derive the first derivative or slope and then use a tangent line to calculate the steepness of this slope. Further, this slope will inform the updates to the parameters (weights and bias). The slope becomes steeper at the starting point or arbitrary point, but whenever new parameters are generated, then steepness gradually reduces, and at the lowest point, it approaches the lowest point, which is called a point of convergence. The main objective of gradient descent is to minimize the cost function or the error between expected and actual. To minimize the cost function, two data points are required: Direction & Learning Rate These two factors are used to determine the partial derivative calculation of future iteration and allow it to the point of convergence or local minimum or global minimum. Let's discuss learning rate factors in brief; Learning Rate: It is defined as the step size taken to reach the minimum or lowest point. This is typically a small value that is evaluated and updated based on the behavior of the cost function. If the learning rate is high, it results in larger steps but also leads to risks of overshooting the minimum. At the same time, a low learning rate shows the small step sizes, which compromises overall efficiency but gives the advantage of more precision. Types of Gradient Descent Based on the error in various training models, the Gradient Descent learning algorithm can be divided into Batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. Let's understand these different types of gradient descent: 1. Batch Gradient Descent: Batch gradient descent (BGD) is used to find the error for each point in the training set and update the model after evaluating all training examples. This procedure is known as the training epoch. In simple words, it is a greedy approach where we have to sum over all examples for each update. Advantages of Batch gradient descent: It produces less noise in comparison to other gradient descent. It produces stable gradient descent convergence. It is Computationally efficient as all resources are used for all training samples. 2. Stochastic gradient descent Stochastic gradient descent (SGD) is a type of gradient descent that runs one training example per iteration. Or in other words, it processes a training epoch for each example within a dataset and updates each training example's parameters one at a time. As it requires only one training example at a time, hence it is easier to store in allocated memory. However, it shows some computational efficiency losses in comparison to batch gradient systems as it shows frequent updates that require more detail and speed. Further, due to frequent updates, it is also treated as a noisy gradient. However, sometimes it can be helpful in finding the global minimum and also escaping the local minimum. Advantages of Stochastic gradient descent: In Stochastic gradient descent (SGD), learning happens on every example, and it consists of a few advantages over other gradient descent. It is easier to allocate in desired memory. It is relatively fast to compute than batch gradient descent. It is more efficient for large datasets. 3. MiniBatch Gradient Descent: Mini Batch gradient descent is the combination of both batch gradient descent and stochastic gradient descent. It divides the training datasets into small batch sizes then performs the updates on those batches separately. Splitting training datasets into smaller batches make a balance to maintain the computational efficiency of batch gradient descent and speed of stochastic gradient descent. Hence, we can achieve a special type of gradient descent with higher computational efficiency and less noisy gradient descent. Advantages of Mini Batch gradient descent: It is easier to fit in allocated memory. It is computationally efficient. It produces stable gradient descent convergence. Challenges with the Gradient Descent Although we know Gradient Descent is one of the most popular methods for optimization problems, it still also has some challenges. There are a few challenges as follows: 1. Local Minima and Saddle Point: For convex problems, gradient descent can find the global minimum easily, while for non-convex problems, it is sometimes difficult to find the global minimum, where the machine learning models achieve the best results. Whenever the slope of the cost function is at zero or just close to zero, this model stops learning further. Apart from the global minimum, there occur some scenarios that can show this slop, which is saddle point and local minimum. Local minima generate the shape similar to the global minimum, where the slope of the cost function increases on both sides of the current points. In contrast, with saddle points, the negative gradient only occurs on one side of the point, which reaches a local maximum on one side and a local minimum on the other side. The name of a saddle point is taken by that of a horse's saddle. The name of local minima is because the value of the loss function is minimum at that point in a local region. In contrast, the name of the global minima is given so because the value of the loss function is minimum there, globally across the entire domain the loss function. 2. Vanishing and Exploding Gradient In a deep neural network, if the model is trained with gradient descent and backpropagation, there can occur two more issues other than local minima and saddle point. Vanishing Gradients: Vanishing Gradient occurs when the gradient is smaller than expected. During backpropagation, this gradient becomes smaller that causing the decrease in the learning rate of earlier layers than the later layer of the network. Once this happens, the weight parameters update until they become insignificant. Exploding Gradient: Exploding gradient is just opposite to the vanishing gradient as it occurs when the Gradient is too large and creates a stable model. Further, in this scenario, model weight increases, and they will be represented as NaN. This problem can be solved using the dimensionality reduction technique, which helps to minimize complexity within the model. Next Topic Machine Learning Experts Salary in India ? prev next ? For Videos Join Our Youtube Channel: Join Now Feedback Send your Feedback to [email protected] Help Others, Please Share Learn Latest Tutorials Splunk SPSS Swagger Transact-SQL Tumblr ReactJS Regex Reinforcement Learning R Programming RxJS React Native Python Design Patterns Python Pillow Python Turtle Keras Preparation Aptitude Reasoning Verbal Ability Interview Questions Company Questions Trending Technologies Artificial Intelligence AWS Selenium Cloud Computing Hadoop ReactJS Data Science Angular 7 Blockchain Git Machine Learning DevOps B.Tech / MCA DBMS Data Structures DAA Operating System Computer Network Compiler Design Computer Organization Discrete Mathematics Ethical Hacking Computer Graphics Software Engineering Web Technology Cyber Security Automata C Programming C++ Java .Net Python Programs Control System Data Mining Data Warehouse Javatpoint Services JavaTpoint offers too many high quality services. Mail us on [email protected], to get more information about given services. Website Designing Website Development Java Development PHP Development WordPress Graphic Designing Logo Digital Marketing On Page and Off Page SEO PPC Content Development Corporate Training Classroom and Online Training Data Entry Training For College Campus JavaTpoint offers college campus training on Core Java, Advance Java, .Net, Android, Hadoop, PHP, Web Technology and Python. Please mail your requirement at [email protected] Duration: 1 week to 2 week Like/Subscribe us for latest updates or newsletter Learn Tutorials Learn Java Learn Data Structures Learn C Programming Learn C++ Tutorial Learn C# Tutorial Learn PHP Tutorial Learn HTML Tutorial Learn JavaScript Tutorial Learn jQuery Tutorial Learn Spring Tutorial Our Websites Javatpoint.com Hindi100.com Lyricsia.com Quoteperson.com Jobandplacement.com Our Services Website Development Android Development Website Designing Digital Marketing Summer Training Industrial Training College Campus Training Contact Address: G-13, 2nd Floor, Sec-3 Noida, UP, 201301, India Contact No: 0120-4256464, 9990449935 Contact Us Subscribe Us Privacy Policy Sitemap About Me ? Copyright 2011-2021 www.javatpoint.com. All rights reserved. Developed by JavaTpoint.
